{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chipojaya1/myNEBDHub/blob/main/DSP_Movie_Reviews_Analysis_Chipo_Jaya.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TntJ6WXFZPdo"
      },
      "source": [
        "\n",
        "\n",
        "<h1 align=\"center\">\n",
        "    NSDC Data Science Projects\n",
        "</h1>\n",
        "  \n",
        "<h2 align=\"center\">\n",
        "    Project: Sentiment Analysis of Movie Reviews\n",
        "</h2>\n",
        "\n",
        "<h3 align=\"center\">\n",
        "    Name: Chipo Jaya\n",
        "</h3>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKnVUiwQZPdq"
      },
      "source": [
        "### **Please read before you begin your project**\n",
        "\n",
        "**Instructions: Google Colab Notebooks:**\n",
        "\n",
        "Google Colab is a free cloud service. It is a hosted Jupyter notebook service that requires no setup to use, while providing free access to computing resources. We will be using Google Colab for this project.\n",
        "\n",
        "Certain parts of this project will be completed individually, while other parts are encouraged to be completed with the rest of your team. In order to work within the Google Colab Notebook, **please start by clicking on \"File\" and then \"Save a copy in Drive.\"** This will save a copy of the notebook in your personal Google Drive. Each member of your team should work on their personal copy.\n",
        "\n",
        "Please rename the file to \"DSP - Movie Reviews Analysis - Your Full Name.\" Once this project is completed, you will be prompted to share your file with the National Student Data Corps (NSDC) Project Leaders.\n",
        "\n",
        "You can now start working on the project. :)\n",
        "\n",
        "**Project Description:**\n",
        "\n",
        "This project will introduce students to an array of skills as they strive to create a sentiment analysis model to classify a given review as positive or negative. Sentiment Analysis leverages both Natural Language Processing and Machine Learning skills - how to represent text in a machine-understandable format so as to classify the text and extract sentiment. We will also cover visualizations and how to deploy models in the real world."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS9ba7_n0aKd"
      },
      "source": [
        "[Use this link to join the NSDC DSP Slack Channel!](https://bit.ly/nsdc-dsp-movie-reviews)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mty7kOOZPdq"
      },
      "source": [
        "\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQpujHnvZPdr"
      },
      "source": [
        "\n",
        "\n",
        "<h3 align = \"center\">\n",
        "    Milestone #1\n",
        "</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e417MmhSZPdr"
      },
      "source": [
        "NOTE: These steps are to be completed **individually**, not as a team. You are encouraged to discuss steps with your teammates. Please attend Office Hours or ask your questions on Slack.\n",
        "\n",
        "GOAL: The main goal of this milestone is to set up your environment, install the required packages, learn how to acces data and do some basic exploratory data analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmouMnaaZPdr"
      },
      "source": [
        "**Step 1:**\n",
        "\n",
        "Setting up libraries and installing packages\n",
        "\n",
        "To install a library:\n",
        "```python\n",
        " import <library> as <shortname>\n",
        "```\n",
        "We use a *short name* since it is easier to refer to the package to access functions and also to refer to subpackages within the library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "le2dKHwnZPds"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy1yhZqNZPdt"
      },
      "source": [
        "These are the libraries that will help us throughout this project. It is not necessary that you know what each library does, but you can always look it up.\n",
        "\n",
        "We encourage you to read more about the important and most commonly used packages like Pandas and Natural Language Toolkit (NLTK) and write a few lines in your own words about what they do. [You may use the Data Science Resource Repository (DSRR) to find resources to get started!](https://nebigdatahub.org/nsdc/data-science-resource-repository/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGftlCsOZPdt"
      },
      "source": [
        "<h4 style=\"color:orange\">\n",
        "    TO-DO\n",
        "</h4>\n",
        "\n",
        "Write a few lines about what each library does.\n",
        "\n",
        "- **Pandas:** A library for data manipulation which is used to load data from excel files, clean it, explore it, and prepare it for the next steps. It's the foundation for organizing and managing textual data efficiently.\n",
        "\n",
        "\n",
        "- **NLTK:** a library called Natural Language Toolkit (NLTK) and it is a comprehensive library for working with human language data (text). It comes with built-in resources like sentiment lexicons (e.g., VADER) and can do and so much more:\n",
        "\n",
        "   1. Tokenization: Splitting sentences and paragraphs into individual words or tokens.\n",
        "\n",
        "    2. Stopword Removal: Filtering out common but low-meaning words like \"the,\" \"is,\" and \"and.\"\n",
        "\n",
        "    3. Stemming/Lemmatization: Reducing words to their base or root form (e.g., \"running\" becomes \"run\").\n",
        "\n",
        "    4. Part-of-Speech Tagging: Identifying nouns, verbs, adjectives, etc., which can be useful for analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ua4f1MgZPdt"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dInyzwncZPdt"
      },
      "source": [
        "**Step 2:**\n",
        "\n",
        "Letâ€™s access our data. We will be using the Internet Movie Database (IMDb) as our dataset. The dataset contains 50,000 movie reviews from the Internet Movie Database. Reviews have been pre-labeled with sentiment polarity (positive/negative).  \n",
        "\n",
        "\n",
        "[The IMDb Movie Reviews dataset is available at this link](https://raw.githubusercontent.com/meghjoshii/NSDC_DataScienceProjects_SentimentAnalysis/main/IMDB%20Dataset.csv). It is better to use the link provided directly within the read_csv function.\n",
        "\n",
        "\n",
        "\n",
        "We will use pandas to read the data from the csv file using the `read_csv` function. This function returns a pandas dataframe. We will store this dataframe in a variable called `df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBDt0Qvzndex",
        "outputId": "1fea6964-bf69-445c-ae40-c0a7a645b2fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded from url!\n",
            "Dataset shape: (50000, 2)\n",
            "Number of reviews: 50,000\n",
            "Number of columns: 2\n"
          ]
        }
      ],
      "source": [
        "# TODO: Read the data using pandas read_csv function\n",
        "\n",
        "# Load the dataset from the provided URL\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/meghjoshii/NSDC_DataScienceProjects_SentimentAnalysis/main/IMDB%20Dataset.csv')\n",
        "\n",
        "print(\"Dataset loaded from url!\")\n",
        "print(f\"Dataset shape: {df.shape}\")  # (number_of_reviews, number_of_columns)\n",
        "print(f\"Number of reviews: {df.shape[0]:,}\")\n",
        "print(f\"Number of columns: {df.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "721YSkyhZPd3"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhFDTTsSZPd4"
      },
      "source": [
        "**Step 3:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BRrwQ2yZPd4"
      },
      "source": [
        "Let's see what the data looks like. We can use the `head` function which returns the first 5 rows of the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "24duiI4RpNDN",
        "outputId": "1bffb400-2e2a-4ccc-a593-0ce1e445426d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 reviews\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-60a61538-ec41-4bd6-b973-ecfa789eac41\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60a61538-ec41-4bd6-b973-ecfa789eac41')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-60a61538-ec41-4bd6-b973-ecfa789eac41 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-60a61538-ec41-4bd6-b973-ecfa789eac41');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-50e8e700-8cd0-4fc1-9c3a-56c8b7fd8ff9\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-50e8e700-8cd0-4fc1-9c3a-56c8b7fd8ff9')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-50e8e700-8cd0-4fc1-9c3a-56c8b7fd8ff9 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49582,\n        \"samples\": [\n          \"\\\"Soul Plane\\\" is a horrible attempt at comedy that only should appeal people with thick skulls, bloodshot eyes and furry pawns. <br /><br />The plot is not only incoherent but also non-existent, acting is mostly sub sub-par with a gang of highly moronic and dreadful characters thrown in for bad measure, jokes are often spotted miles ahead and almost never even a bit amusing. This movie lacks any structure and is full of racial stereotypes that must have seemed old even in the fifties, the only thing it really has going for it is some pretty ladies, but really, if you want that you can rent something from the \\\"Adult\\\" section. OK?<br /><br />I can hardly see anything here to recommend since you'll probably have a lot a better and productive time chasing rats with a sledgehammer or inventing waterproof teabags or whatever.<br /><br />2/10\",\n          \"Guest from the Future tells a fascinating story of time travel, friendship, battle of good and evil -- all with a small budget, child actors, and few special effects. Something for Spielberg and Lucas to learn from. ;) A sixth-grader Kolya \\\"Nick\\\" Gerasimov finds a time machine in the basement of a decrepit building and travels 100 years into the future. He discovers a near-perfect, utopian society where robots play guitars and write poetry, everyone is kind to each other and people enjoy everything technology has to offer. Alice is the daughter of a prominent scientist who invented a device called Mielophone that allows to read minds of humans and animals. The device can be put to both good and bad use, depending on whose hands it falls into. When two evil space pirates from Saturn who want to rule the universe attempt to steal Mielophone, it falls into the hands of 20th century school boy Nick. With the pirates hot on his tracks, he travels back to his time, followed by the pirates, and Alice. Chaos, confusion and funny situations follow as the luckless pirates try to blend in with the earthlings. Alice enrolls in the same school Nick goes to and demonstrates superhuman abilities in PE class. The catch is, Alice doesn't know what Nick looks like, while the pirates do. Also, the pirates are able to change their appearance and turn literally into anyone. (Hmm, I wonder if this is where James Cameron got the idea for Terminator...) Who gets to Nick -- and Mielophone -- first? Excellent plot, non-stop adventures, and great soundtrack. I wish Hollywood made kid movies like this one...\",\n          \"\\\"National Treasure\\\" (2004) is a thoroughly misguided hodge-podge of plot entanglements that borrow from nearly every cloak and dagger government conspiracy clich\\u00e9 that has ever been written. The film stars Nicholas Cage as Benjamin Franklin Gates (how precious is that, I ask you?); a seemingly normal fellow who, for no other reason than being of a lineage of like-minded misguided fortune hunters, decides to steal a 'national treasure' that has been hidden by the United States founding fathers. After a bit of subtext and background that plays laughably (unintentionally) like Indiana Jones meets The Patriot, the film degenerates into one misguided whimsy after another \\u0096 attempting to create a 'Stanley Goodspeed' regurgitation of Nicholas Cage and launch the whole convoluted mess forward with a series of high octane, but disconnected misadventures.<br /><br />The relevancy and logic to having George Washington and his motley crew of patriots burying a king's ransom someplace on native soil, and then, going through the meticulous plan of leaving clues scattered throughout U.S. currency art work, is something that director Jon Turteltaub never quite gets around to explaining. Couldn't Washington found better usage for such wealth during the start up of the country? Hence, we are left with a mystery built on top of an enigma that is already on shaky ground by the time Ben appoints himself the new custodian of this untold wealth. Ben's intentions are noble \\u0096 if confusing. He's set on protecting the treasure. For who and when?\\u0085your guess is as good as mine.<br /><br />But there are a few problems with Ben's crusade. First up, his friend, Ian Holmes (Sean Bean) decides that he can't wait for Ben to make up his mind about stealing the Declaration of Independence from the National Archives (oh, yeah \\u0096 brilliant idea!). Presumably, the back of that famous document holds the secret answer to the ultimate fortune. So Ian tries to kill Ben. The assassination attempt is, of course, unsuccessful, if overly melodramatic. It also affords Ben the opportunity to pick up, and pick on, the very sultry curator of the archives, Abigail Chase (Diane Kruger). She thinks Ben is clearly a nut \\u0096 at least at the beginning. But true to action/romance form, Abby's resolve melts quicker than you can say, \\\"is that the Hope Diamond?\\\" The film moves into full X-File-ish mode, as the FBI, mistakenly believing that Ben is behind the theft, retaliate in various benign ways that lead to a multi-layering of action sequences reminiscent of Mission Impossible meets The Fugitive. Honestly, don't those guys ever get 'intelligence' information that is correct? In the final analysis, \\\"National Treasure\\\" isn't great film making, so much as it's a patchwork rehash of tired old bits from other movies, woven together from scraps, the likes of which would make IL' Betsy Ross blush.<br /><br />The Buena Vista DVD delivers a far more generous treatment than this film is deserving of. The anamorphic widescreen picture exhibits a very smooth and finely detailed image with very rich colors, natural flesh tones, solid blacks and clean whites. The stylized image is also free of blemishes and digital enhancements. The audio is 5.1 and delivers a nice sonic boom to your side and rear speakers with intensity and realism. Extras include a host of promotional junket material that is rather deep and over the top in its explanation of how and why this film was made. If only, as an audience, we had had more clarification as to why Ben and co. were chasing after an illusive treasure, this might have been one good flick. Extras conclude with the theatrical trailer, audio commentary and deleted scenes. Not for the faint-hearted \\u0096 just the thick-headed.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"negative\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# TODO: Print the first 5 rows of the data using head function of pandas\n",
        "print(\"First 5 reviews\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnZloVlvZPd4"
      },
      "source": [
        "There are 2 columns in the dataframe - `review` and `sentiment`. The `review` column contains the text of the review and the `sentiment` column contains the sentiment of the review.\n",
        "\n",
        "The `describe()` function gives us a summary of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "IfzpsKR2BIFy",
        "outputId": "d90ef30a-5b1d-451f-a269-44d1519f4d14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset summary\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   review sentiment\n",
              "count                                               50000     50000\n",
              "unique                                              49582         2\n",
              "top     Loved today's show!!! It was a variety and not...  positive\n",
              "freq                                                    5     25000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cae5241c-2b5c-4fd8-9316-e4d491ea1392\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>50000</td>\n",
              "      <td>50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>49582</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>Loved today's show!!! It was a variety and not...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>5</td>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cae5241c-2b5c-4fd8-9316-e4d491ea1392')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cae5241c-2b5c-4fd8-9316-e4d491ea1392 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cae5241c-2b5c-4fd8-9316-e4d491ea1392');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a1a8abbb-e7ad-4f37-bd62-182df1390ec6\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a1a8abbb-e7ad-4f37-bd62-182df1390ec6')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a1a8abbb-e7ad-4f37-bd62-182df1390ec6 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          49582,\n          \"5\",\n          \"50000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          \"25000\",\n          \"50000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# TODO: Describe the data using describe function of pandas\n",
        "print(\"Dataset summary\")\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW0psSI_ZPd5"
      },
      "source": [
        "We can see that we have 50,000 reviews in our dataset. The `sentiment` column has 2 unique values - `positive` and `negative`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT9zPK-tZPd5"
      },
      "source": [
        "Individual columns can be accessed using the `[]` operator. For example, `df['review']` returns the `review` column of the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyqiBi-OZPd5",
        "outputId": "6f6c0661-7b4f-4fd1-fd67-6db4f8f8530d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        One of the other reviewers has mentioned that ...\n",
            "1        A wonderful little production. <br /><br />The...\n",
            "2        I thought this was a wonderful way to spend ti...\n",
            "3        Basically there's a family where a little boy ...\n",
            "4        Petter Mattei's \"Love in the Time of Money\" is...\n",
            "                               ...                        \n",
            "49995    I thought this movie did a down right good job...\n",
            "49996    Bad plot, bad dialogue, bad acting, idiotic di...\n",
            "49997    I am a Catholic taught in parochial elementary...\n",
            "49998    I'm going to have to disagree with the previou...\n",
            "49999    No one expects the Star Trek movies to be high...\n",
            "Name: review, Length: 50000, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(df['review'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxbGYJZIZPd5"
      },
      "source": [
        "Let's see how many positive and negative reviews we have in our dataset. We can use the `value_counts()` function to get the count of each unique value in the `sentiment` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DbvKXDBBWvW",
        "outputId": "5266195f-54f0-4fd0-de22-74cde0d3a8c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of positive and negative reviews\n",
            "sentiment\n",
            "positive    25000\n",
            "negative    25000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Sentiment distribution (percentages)\n",
            "sentiment\n",
            "positive    50.0\n",
            "negative    50.0\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# TODO: Use the value_counts function to count the number of positive and negative reviews on the sentiment column using the [] operator\n",
        "print(\"Number of positive and negative reviews\")\n",
        "print(df['sentiment'].value_counts())\n",
        "\n",
        "print(\"\\nSentiment distribution (percentages)\")\n",
        "print(df['sentiment'].value_counts(normalize=True) * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwbdUzT8ZPd5"
      },
      "source": [
        "We can see that we have 25,000 positive reviews and 25,000 negative reviews in our dataset. They are evenly distributed and we do not have to worry about class imbalance.\n",
        "\n",
        "[Follow this link to learn more about class imbalance](https://machinelearningmastery.com/what-is-imbalanced-classification/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtajsRi-xRCm",
        "outputId": "8d62a3e7-3b13-438e-f3f3-bd61a7582d55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sample Reviews ---\n",
            "Sample positive review:\n",
            "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ...\n",
            "\n",
            "Sample negative review:\n",
            "Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins ...\n",
            "\n",
            "--- Review length statistics ---\n",
            "Average review length: 1309 characters\n",
            "Shortest review: 32 characters\n",
            "Longest review: 13704 characters\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Sample Reviews ---\")\n",
        "print(\"Sample positive review:\")\n",
        "print(df[df['sentiment'] == 'positive']['review'].iloc[0][:500] + \"...\")  # First 500 chars\n",
        "\n",
        "print(\"\\nSample negative review:\")\n",
        "print(df[df['sentiment'] == 'negative']['review'].iloc[0][:500] + \"...\")  # First 500 chars\n",
        "\n",
        "print(\"\\n--- Review length statistics ---\")\n",
        "df['review_length'] = df['review'].str.len()\n",
        "print(f\"Average review length: {df['review_length'].mean():.0f} characters\")\n",
        "print(f\"Shortest review: {df['review_length'].min()} characters\")\n",
        "print(f\"Longest review: {df['review_length'].max()} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT26TL00ZPd5"
      },
      "source": [
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0mDeS4HZPd5"
      },
      "source": [
        "**Step 4:**\n",
        "Let's take a short break from coding and do some reading that is imperative to understand the concepts of this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ2xYujeZPd6"
      },
      "source": [
        "\n",
        "The **objective** of our machine learning model will be to predict the sentiment of a review given the text of the review. So, the model needs to learn the relationship between the text of the review and the sentiment of the review. Hence, this is a supervised learning problem where the input is text and the output is a label.\n",
        "\n",
        "[Click here to watch introductory videos and learn more about supervised machine learning](https://www.youtube.com/playlist?list=PLNs9ZO9jGtUCiGTo3iP0qmI9_qi8oYaRN).\n",
        "\n",
        "\n",
        "\n",
        "Since we are going to be using text as input, we cannot directly use the text because computers do not understand text. We need to convert the text into a format that is useful for our classification model.\n",
        "\n",
        "Count vectorization is a method to convert text into a format that is useful for classification models. It converts the text into a matrix of token counts meaning that each row in the matrix represents a review and each column represents a word. The value in each cell is the number of times that word occurs in that review. So, by learning the frequency of each word in each review, the model can learn the relationship between the text and the sentiment of the review. The intuition behind this is that positive reviews will have more positive words and negative reviews will have more negative words.\n",
        "\n",
        "Now that we have established the intuition behind count vectorization, let's look at features of the count vectorizer. The features of the count vectorizer are the words that we want to consider. We would only want to use words that are relevant to the sentiment of the review. For example, if we are classifying reviews of movies, we would not want to consider words like `the`, `a`, `an` etc. because they are not relevant to the sentiment of the review. Also, we would want to consider words that occur frequently in the reviews. For example, if a word occurs only once in the entire dataset, it is not very useful for our model.\n",
        "\n",
        "To remove words that are not relevant to the sentiment of the review, first we need to tokenize the text.\n",
        "\n",
        "Tokenization is the process of splitting a string into a list of tokens. This helps us to break down the text into smaller chunks which are easier to work with. What we essentially want to do  is remove all the punctuation and special characters from the text because they do not add any value to the text. We also want to convert all the text to lowercase so that the model does not treat the same word with different cases as different words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7X5T79mZPd6"
      },
      "source": [
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3NohCaoZPd6"
      },
      "source": [
        "\n",
        "\n",
        "<h3 align = \"center\">\n",
        "    Milestone #2\n",
        "</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9hFI6uKZPd6"
      },
      "source": [
        "NOTE: These steps are to be completed **individually**, not as a team. You are encouraged to discuss steps with your teammates. Please attend Office Hours or ask your questions on Slack.\n",
        "\n",
        "GOAL: The main goal of this milestone is to learn natural langauge processing and how to use the NLTK library to preprocess text. We will also learn how to use the CountVectorizer class to convert text into a format that is useful for classification models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1uBbf33ZPd6"
      },
      "source": [
        "**Step 1:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtYBV0jQZPd6"
      },
      "source": [
        "We will use the `nltk` library to perform these preprocessing steps. First, we will use the `word_tokenize` function to tokenize the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NFQtgeAZPd6",
        "outputId": "47e9f38f-414c-411e-daa6-da8a9b21e30d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the punkt tokenizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Download the missing 'punkt_tab' resource\n",
        "\n",
        "# Apply word_tokenize to the 'review' column and create a new column for tokens\n",
        "df['review_tokens'] = df['review'].apply(word_tokenize)\n",
        "\n",
        "print(\"Tokenization completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4TsDmstZPd6"
      },
      "outputs": [],
      "source": [
        "# We can see that the `review` column now contains a list of tokens for each review. Let's see what the first review looks like.\n",
        "df['review_tokens'][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uwzXgWcZPd6"
      },
      "source": [
        "We see that the text has been tokenized into a list of words. Also, the list contains punctuation and special characters which we do not want."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4GGKW28ZPd7"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtqjXDhAZPd7"
      },
      "source": [
        "**Step 2:**\n",
        "\n",
        "\n",
        "Let's clean the text by removing punctuations, special characters and converting the text to lowercase. We will use the `isalpha` function to check if a word is an alphabet. If it is not an alphabet, we will remove it from the list. We will also convert the text to lowercase using the `lower` function. Next, we will remove the stopwords from the list. Stopwords are words that do not add any value to the text. For example, `the`, `a`, `an` etc. are stopwords. We will use the `stopwords` function from the `nltk.corpus` package to get a list of stopwords. We will then use the `remove` function to remove the stopwords from the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmMAl_YnZPd7"
      },
      "outputs": [],
      "source": [
        "# isalpha() function returns True if all the characters in the string are alphabets. If not, it returns False.\n",
        "\n",
        "# We can use the isalpha() function to remove all the punctuations and numbers from the reviews.\n",
        "\n",
        "# Remove non-alphabetic tokens using isalpha()\n",
        "df['review_tokens_clean'] = df['review_tokens'].apply(\n",
        "    lambda tokens: [token for token in tokens if token.isalpha()]\n",
        ")\n",
        "\n",
        "print(\"Non-alphabetic tokens removed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_J_DyzyZPd7"
      },
      "outputs": [],
      "source": [
        "print(\" \".join(df['review_tokens_clean'][1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egtHLhV_ZPd7"
      },
      "outputs": [],
      "source": [
        "#TODO: convert to lowercase\n",
        "#complete the code below\n",
        "#df[''] = df[''].apply(lambda x: [item. for item in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACEl3_P79Wl4"
      },
      "outputs": [],
      "source": [
        "df['review_tokens_lower'] = df['review_tokens_clean'].apply(lambda x: [item.lower() for item in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IKcYhFjZPd7"
      },
      "outputs": [],
      "source": [
        "# remove stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords if you haven't already\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Get the English stopwords list\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "print(f\"Number of stopwords: {len(stop_words)}\")\n",
        "print(f\"Sample stopwords: {list(stop_words)[:10]}\")\n",
        "\n",
        "# Remove stopwords from the lowercase tokens\n",
        "df['review_tokens_no_stopwords'] = df['review_tokens_lower'].apply(\n",
        "    lambda tokens: [token for token in tokens if token not in stop_words]\n",
        ")\n",
        "\n",
        "print(\"Stopwords removed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TxoAfuaZPd7"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOvMGYcCZPd7"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqEiNV5uZPd8"
      },
      "source": [
        "**Step 3:**\n",
        "\n",
        "Now that we have cleaned the text, we need to use a stemmer to stem the words. Stemming is the process of reducing a word to its root form. For example, the root form of the word `running` is `run`. Stemming helps us to reduce the number of unique words in the text. We will use the `PorterStemmer` function from the `nltk.stem` package to stem the words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NqS01znZPd8"
      },
      "outputs": [],
      "source": [
        "# stemming user PorterStemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the Porter Stemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "print(\"Porter Stemmer initialized!\")\n",
        "\n",
        "# Apply stemming to the tokens (using the stopword-removed version)\n",
        "df['review_tokens_stemmed'] = df['review_tokens_no_stopwords'].apply(\n",
        "    lambda tokens: [ps.stem(token) for token in tokens]\n",
        ")\n",
        "\n",
        "print(\"Stemming completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86zNoUceZPd8"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRDONNgaZPd8"
      },
      "outputs": [],
      "source": [
        "#join list of words to form sentences\n",
        "df['review_cleaned'] = df['review_tokens_stemmed'].apply(\n",
        "    lambda tokens: ' '.join(tokens)\n",
        ")\n",
        "\n",
        "print(\"Tokens joined back into sentences!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeOaUm0MZPd8"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdA5N6yH4jDY"
      },
      "outputs": [],
      "source": [
        "# Compare original vs cleaned reviews\n",
        "print(\"--- Original Review Sample ---\")\n",
        "print(df['review'].iloc[0][:300] + \"...\")\n",
        "\n",
        "print(\"\\n--- Cleaned Review Sample ---\")\n",
        "print(df['review_cleaned'].iloc[0][:300] + \"...\")\n",
        "\n",
        "print(f\"\\nOriginal review length: {len(df['review'].iloc[0])} characters\")\n",
        "print(f\"Cleaned review length: {len(df['review_cleaned'].iloc[0])} characters\")\n",
        "\n",
        "# Show the complete preprocessing pipeline for one review\n",
        "print(\"\\n--- Complete Preprocessing Pipeline for One Review ---\")\n",
        "print(\"Original:\", df['review'].iloc[0][:150] + \"...\")\n",
        "print(\"Tokenized:\", df['review_tokens'].iloc[0][:10])\n",
        "print(\"Cleaned (alpha only):\", df['review_tokens_clean'].iloc[0][:10])\n",
        "print(\"Lowercase:\", df['review_tokens_lower'].iloc[0][:10])\n",
        "print(\"No stopwords:\", df['review_tokens_no_stopwords'].iloc[0][:10])\n",
        "print(\"Stemmed:\", df['review_tokens_stemmed'].iloc[0][:10])\n",
        "print(\"Final joined:\", df['review_cleaned'].iloc[0][:150] + \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIaorSjD4wz8"
      },
      "outputs": [],
      "source": [
        "# Create cleaned versions from different preprocessing stages\n",
        "df['review_no_stopwords'] = df['review_tokens_no_stopwords'].apply(lambda x: ' '.join(x))\n",
        "df['review_lower_clean'] = df['review_tokens_lower'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "print(\"Multiple cleaned versions created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_kHqRKY40sM"
      },
      "outputs": [],
      "source": [
        "print(\"--- Final Dataset Overview ---\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(\"\\nColumns available:\")\n",
        "print(df.columns.tolist())\n",
        "\n",
        "print(\"\\n--- Sample of Final Cleaned Reviews ---\")\n",
        "for i in range(2):\n",
        "    print(f\"\\nReview {i+1}:\")\n",
        "    print(f\"Sentiment: {df['sentiment'].iloc[i]}\")\n",
        "    print(f\"Cleaned text: {df['review_cleaned'].iloc[i][:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grdrqrdoZPd8"
      },
      "source": [
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhRdCZx7ZPd8"
      },
      "source": [
        "\n",
        "\n",
        "<h3 align = \"center\">\n",
        "    Milestone #3\n",
        "</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiA7qc95ZPd8"
      },
      "source": [
        "NOTE: These steps are to be completed **individually**, not as a team. You are encouraged to discuss steps with your teammates. Please attend Office Hours or ask your questions on Slack.\n",
        "\n",
        "GOAL: The main goal of this milestone is to split the dataset into training and testing sets. We will also learn how to use the CountVectorizer class to convert text into a format that is useful for classification models. We will also learn how to use the MultinomialNB class to train a Naive Bayes classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep1BAYMi4qmf"
      },
      "source": [
        "Training and Testing Data:\n",
        "\n",
        "Machine learning uses algorithms to learn from data in datasets. They find patterns, develop understanding, make decisions, and evaluate those decisions.\n",
        "\n",
        "In machine learning, datasets are split into two subsets:\n",
        "\n",
        "The first subset is known as the **training data** - itâ€™s a portion of our actual dataset that is fed into the machine learning model to discover and learn patterns. In this way, it trains our model.\n",
        "\n",
        "The other subset is known as the **testing data**.\n",
        "\n",
        "Once your machine learning model is built (with your training data), you need unseen data to test your model. This data is called testing data, and you can use it to evaluate the performance and progress of your algorithmsâ€™ training and adjust or optimize it for improved results.\n",
        "\n",
        "Testing data has two main criteria. It should:\n",
        "\n",
        "1. Represent the actual dataset\n",
        "2. Be large enough to generate meaningful predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TGKhOnjZPd8"
      },
      "source": [
        "**Step 1:**\n",
        "\n",
        "Now, the data is tokenized, cleaned and reduced to its root form.\n",
        "The next step is to split the data for training and testing. We split the data because we need to train our model on some data and test it on some data. We have a total of 50,000 reviews, so let's split it into 40,000 reviews for training and 10,000 reviews for testing.\n",
        "To do this, we can use the slice operator `:`. For example, `df[:30000]` returns the first 30,000 rows of the dataframe. Similarly, `df[30000:]` returns the last 20,000 rows of the dataframe.\n",
        "\n",
        "Name the training data as `train_reviews` and testing data as `test_reviews`. Remember, we are only splitting the reviews column and will do the same for sentiment in the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CwLp2sDCL8M"
      },
      "outputs": [],
      "source": [
        "# Split the reviews into training and testing sets using slicing\n",
        "\n",
        "#train reviews\n",
        "train_reviews = df['review_cleaned'][:40000]  # First 40,000 reviews for training\n",
        "print(f\"Training reviews shape: {train_reviews.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDlrQ-1DCPrS"
      },
      "outputs": [],
      "source": [
        "# Split the reviews into training and testing sets using slicing\n",
        "\n",
        "# test reviews\n",
        "test_reviews = df['review_cleaned'][40000:]   # Last 10,000 reviews for testing\n",
        "print(f\"Testing reviews shape: {test_reviews.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p596V8gXZPd9"
      },
      "source": [
        "Now let us do the same for the sentiment column. Name the training data as `train_sentiments` and testing data as `test_sentiments`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0GW7b7CZPd9"
      },
      "outputs": [],
      "source": [
        "#TODO: train sentiments\n",
        "train_sentiments = df['sentiment'][:40000]  # First 40,000 sentiments for training\n",
        "print(f\"Training sentiments shape: {train_sentiments.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwiOlI_ZZPd9"
      },
      "outputs": [],
      "source": [
        "#TODO: test sentiments\n",
        "test_sentiments = df['sentiment'][40000:]   # Last 10,000 sentiments for testing\n",
        "print(f\"Testing sentiments shape: {test_sentiments.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txlHBwRG6JDB"
      },
      "outputs": [],
      "source": [
        "# Verification of the distribution in both sets\n",
        "print(\"\\n--- Training Set Sentiment Distribution ---\")\n",
        "print(train_sentiments.value_counts())\n",
        "\n",
        "print(\"\\n--- Testing Set Sentiment Distribution ---\")\n",
        "print(test_sentiments.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvy4Ng5IZPd9"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYIHq7__ZPd9"
      },
      "source": [
        "**Step 2:**\n",
        "\n",
        "We need to make a few changes to the data before we can use it to train our model. First, we need to convert the data into a format that is useful for our model. We will use the `CountVectorizer` function from the `sklearn.feature_extraction.text` package to convert the text into a matrix of token counts.\n",
        "\n",
        "For the sentiment column, we need to convert the labels into numbers. We will use the `LabelEncoder` function from the `sklearn.preprocessing` package to convert the labels into numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs5GfcJRZPd9"
      },
      "source": [
        "[To read more about Count Vectorizer, follow this link](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). [You can also use this link to read more about Label Encoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html). Please go through the parameters of both these functions to better understand the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxzTuYbmCShZ"
      },
      "outputs": [],
      "source": [
        "#Count vectorizer for bag of words\n",
        "cv = CountVectorizer(min_df=1, max_df=1.0, binary=False, ngram_range=(1,3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x_082TdZPd9"
      },
      "source": [
        "To transform the data, we will use the `fit_transform` function. The `fit_transform` function fits the model to the data and then transforms the data into a matrix of token counts. We will use the `fit_transform` function on the training data and the `transform` function on the testing data. This is because we only want to fit the model to the training data and not the testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43Thg7oeZPd9"
      },
      "outputs": [],
      "source": [
        "#transformed train reviews\n",
        "cv_train_reviews = cv.fit_transform(train_reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQP_nC4CZPd9"
      },
      "outputs": [],
      "source": [
        "#transformed test reviews\n",
        "cv_test_reviews = cv.transform(test_reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdinQo9o7Rel"
      },
      "outputs": [],
      "source": [
        "print(\"CountVectorizer transformation completed!\")\n",
        "print(f\"Training features shape: {cv_train_reviews.shape}\")\n",
        "print(f\"Testing features shape: {cv_test_reviews.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWxxbASGZPd9"
      },
      "source": [
        "Again, for the sentiment column, we will use the `fit_transform` function on the training data and the `transform` function on the testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9xLTsunCcdI"
      },
      "outputs": [],
      "source": [
        "#labeling the sentient data\n",
        "lb = LabelBinarizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKCUVO5oZPd-"
      },
      "outputs": [],
      "source": [
        "# transformed sentiment data\n",
        "lb_train_sentiments = lb.fit_transform(train_sentiments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5lYIr4XZPd-"
      },
      "outputs": [],
      "source": [
        "#TODO: transformed test sentiment data (similar to count vectorizer, transform test reviews, name it lb_test_sentiments)\n",
        "lb_test_sentiments = lb.transform(test_sentiments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGuYMd6c7au9"
      },
      "outputs": [],
      "source": [
        "print(\"Label binarization completed!\")\n",
        "print(f\"Training labels shape: {lb_train_sentiments.shape}\")\n",
        "print(f\"Testing labels shape: {lb_test_sentiments.shape}\")\n",
        "print(f\"Label classes: {lb.classes_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkxY_9XRZPd-"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1RZAqvQZPd-"
      },
      "source": [
        "**Step 3:**\n",
        "\n",
        "Model Building: In this step, we will build our model. We will use the `MultinomialNB` function from the `sklearn.naive_bayes` package to build our model. The Multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. Bag-of-Word counts are an example of integer-valued discrete features.\n",
        "\n",
        "[Please read about the Multinomial Naive Bayes classifier here](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) and write about it in the comments below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJilZDoDZPd-"
      },
      "source": [
        "<h4 style=\"color:orange\">\n",
        "    TO-DO\n",
        "</h4>\n",
        "\n",
        "Write a few lines about the following:\n",
        "\n",
        "- **Machine Learning Classifiers:** Algorithms that automatically learn to categorize data into predefined classes or labels based on patterns and features in the training data. They work by finding relationships between input features and output labels, then using these learned patterns to predict labels for new, unseen data. Common classifiers include Naive Bayes, Logistic Regression, Support Vector Machines, and Decision Trees.\n",
        "\n",
        "- **Naive Bayes Classifier:** A probabilistic classification algorithm based on Bayes' theorem with a \"naive\" assumption of feature independence. It calculates the probability of each class given the input features and selects the class with the highest probability. Despite its simplifying assumption (that all features are independent), it also works for text classification tasks like sentiment analysis and spam detection because it's fast, efficient, and performs well with high-dimensional data like text features.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWxBnsi-ZPd-"
      },
      "source": [
        "The first part is to train and fit the Multinomial Naive Bayes classifier to the training data. We will use the `fit` function to train the model on the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rI4TpFtOCke6"
      },
      "outputs": [],
      "source": [
        "# training the model\n",
        "mnb = MultinomialNB()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vK7QbHmUZPd-"
      },
      "outputs": [],
      "source": [
        "# fitting the model\n",
        "mnb_bow = mnb.fit(cv_train_reviews, lb_train_sentiments.ravel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oj9NLo4ZCr7V"
      },
      "outputs": [],
      "source": [
        "#Predicting the model for bag of words\n",
        "mnb_bow_predict = mnb.predict(cv_test_reviews)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "950v5Y4YCveb"
      },
      "outputs": [],
      "source": [
        "#Accuracy score for bag of words\n",
        "mnb_bow_score = accuracy_score(lb_test_sentiments, mnb_bow_predict)\n",
        "print(\"Accuracy :\", mnb_bow_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fKVYr6zZPd-"
      },
      "source": [
        "This means the model correctly classified 88.48% of the 10,000 test reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jhsah2fiZPd-"
      },
      "source": [
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQLLgAgcZPd_"
      },
      "source": [
        "<h3 align = \"center\">\n",
        "    Milestone #4\n",
        "</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6E5VHGMZPd_"
      },
      "source": [
        "NOTE: This milestone is to be completed as a **group**.  Each group member should try a different classifier and you must discuss the results with your teammates. Please attend Office Hours or ask your questions on Slack.\n",
        "\n",
        "GOAL: The main goal of this milestone is to understand how to use different classifiers to train a model and how to evaluate the performance of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kVj97SAZPd_"
      },
      "source": [
        "**Step 1:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFRvyZl8ZPd_"
      },
      "source": [
        "[Please read about the different classifiers here](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning).\n",
        "Each team member should try at least one **different** classifier. You can try more than one classifier if you want. Please write about the classifier you have chosen in the comments below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M59jccXWZPd_"
      },
      "source": [
        "<h4 style=\"color:orange\">\n",
        "    TO-DO\n",
        "</h4>\n",
        "\n",
        "**Classifiers chosen**:\n",
        "1. **Logistic Regression:**  Logistic Regression is a fundamental linear classifier that works well for text classification. It's fast, interpretable, and provides probability estimates. For sentiment analysis, it can learn which words contribute most to positive/negative sentiment.\n",
        "\n",
        "    Expected results: Typically performs very well on text data, often comparable to or better than Naive Bayes.\n",
        "\n",
        "2. **Linear Support Vector Machine (SVM):** SVMs are powerful for high-dimensional data like text. They find the optimal decision boundary that maximizes the margin between classes. LinearSVC is efficient for large-scale text classification.\n",
        "\n",
        "    Expected results: Often achieves state-of-the-art performance on text classification tasks, though can be slower to train.\n",
        "\n",
        "3. **Random Forest:** As an ensemble method, Random Forest combines multiple decision trees. It's robust to overfitting and can capture complex non-linear relationships. It provides feature importance scores.\n",
        "\n",
        "    Expected results: May perform well but could be slower and use more memory than linear models for text data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRGuYLQEBkvY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Dictionary to store results\n",
        "classifier_results = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAAPJ8P9BWI9"
      },
      "outputs": [],
      "source": [
        "# Classifier 1: Logistic Regression\n",
        "print(\"=\" * 60)\n",
        "print(\"CLASSIFIER 1: LOGISTIC REGRESSION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# training the model\n",
        "lr = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "# fitting the model\n",
        "lr.fit(cv_train_reviews, lb_train_sentiments.ravel())\n",
        "\n",
        "# Predict the model\n",
        "lr_predict = lr.predict(cv_test_reviews)\n",
        "\n",
        "# Accuracy score\n",
        "lr_acc = accuracy_score(lb_test_sentiments, lr_predict)\n",
        "\n",
        "print(f\"Accuracy: {lr_acc:.4f} ({lr_acc*100:.2f}%)\")\n",
        "classifier_results['Logistic Regression'] = lr_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nfcXphPUBWAf"
      },
      "outputs": [],
      "source": [
        "# Classifier 2: Linear Support Vector Machine (SVM)\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CLASSIFIER 2: LINEAR SUPPORT VECTOR MACHINE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# training the model\n",
        "svm = LinearSVC(random_state=42, max_iter=2000)\n",
        "\n",
        "# fitting the model\n",
        "svm.fit(cv_train_reviews, lb_train_sentiments.ravel())\n",
        "\n",
        "# Predict the model\n",
        "svm_predict = svm.predict(cv_test_reviews)\n",
        "\n",
        "# Accuracy score\n",
        "svm_acc = accuracy_score(lb_test_sentiments, svm_predict)\n",
        "\n",
        "print(f\"Accuracy: {svm_acc:.4f} ({svm_acc*100:.2f}%)\")\n",
        "classifier_results['Linear SVM'] = svm_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vsHUPl-SBV9R"
      },
      "outputs": [],
      "source": [
        "# Classifier 3: Random Forest\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CLASSIFIER 3: RANDOM FOREST\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# training the model\n",
        "rf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
        "\n",
        "# fitting the model\n",
        "rf.fit(cv_train_reviews, lb_train_sentiments.ravel())\n",
        "\n",
        "# Predict the model\n",
        "rf_predict = rf.predict(cv_test_reviews)\n",
        "\n",
        "# Accuracy score\n",
        "rf_acc = accuracy_score(lb_test_sentiments, rf_predict)\n",
        "\n",
        "print(f\"Accuracy: {rf_acc:.4f} ({rf_acc*100:.2f}%)\")\n",
        "classifier_results['Random Forest'] = rf_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hBGkNG__ZPd_"
      },
      "outputs": [],
      "source": [
        "# Compare all classifiers to Naive Bayes\n",
        "classifier_results['Naive Bayes'] = mnb_bow_score\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CLASSIFIER COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "for classifier, accuracy in sorted(classifier_results.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{classifier:20}: {accuracy:.4f} ({accuracy*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nYvauEJJEFY1"
      },
      "outputs": [],
      "source": [
        "# Detailed evaluation for each classifier\n",
        "classifiers = {\n",
        "    'Logistic Regression': (lr, lr_predict),\n",
        "    'Linear SVM': (svm, svm_predict),\n",
        "    'Random Forest': (rf, rf_predict),\n",
        "    'Naive Bayes': (mnb_bow, mnb_bow_predict)\n",
        "}\n",
        "\n",
        "for name, (classifier, predictions) in classifiers.items():\n",
        "    print(f\"\\n{name} - Detailed Classification Report:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(classification_report(lb_test_sentiments, predictions, target_names=lb.classes_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr9E1ee1ZPd_"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTCklEYkZPd_"
      },
      "source": [
        "**Step 2:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB5Wc7FiZPd_"
      },
      "source": [
        "Data Visualizations are a great way to understand the data. [If you are interested in finding additional resources on Data Visualization, we recommend leveraging the DSRR resources, linked here.](https://nebigdatahub.org/nsdc/data-science-resource-repository/) We will be using word clouds to visualize the data. Word clouds are a great way to visualize the most frequent words in a text. We will use the `WordCloud` function from the `wordcloud` package to visualize the most frequent words in the reviews.\n",
        "\n",
        "[Please read about the WordCloud function here](https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zfajmPP3C6ZZ"
      },
      "outputs": [],
      "source": [
        "# word cloud for positive review words in the entire dataset\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#join all the positive reviews\n",
        "positive_words = ' '.join(list(df[df['sentiment'] == 'positive']['review_cleaned']))\n",
        "\n",
        "#word cloud for positive words\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(positive_words)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bhEL3IO3C9O4"
      },
      "outputs": [],
      "source": [
        "# TODO: Word cloud for negative reviews in the dataset\n",
        "# Word cloud for negative review words\n",
        "negative_words = ' '.join(list(df[df['sentiment'] == 'negative']['review_cleaned']))\n",
        "\n",
        "# Word cloud for negative words\n",
        "wordcloud_negative = WordCloud(\n",
        "    width=800,\n",
        "    height=500,\n",
        "    random_state=21,\n",
        "    max_font_size=110,\n",
        "    background_color='white',\n",
        "    colormap='Reds'\n",
        ").generate(negative_words)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(wordcloud_negative, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title('Most Frequent Words in Negative Reviews', fontsize=16, pad=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iYrntNboES0f"
      },
      "outputs": [],
      "source": [
        "# Visualization of classifier performance\n",
        "plt.figure(figsize=(10, 6))\n",
        "classifiers_names = list(classifier_results.keys())\n",
        "accuracies = list(classifier_results.values())\n",
        "\n",
        "bars = plt.bar(classifiers_names, accuracies, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])\n",
        "plt.ylim(0.8, 0.9)  # Focus on the range where our accuracies lie\n",
        "plt.title('Classifier Performance Comparison on Sentiment Analysis')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Classifiers')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, accuracy in zip(bars, accuracies):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002,\n",
        "             f'{accuracy:.4f}', ha='center', va='bottom')\n",
        "\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3q609j9Elau"
      },
      "outputs": [],
      "source": [
        "# Display cleaned word clouds\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "ax1.imshow(wordcloud_positive, interpolation=\"bilinear\")\n",
        "ax1.set_title('Positive Reviews - Cleaned Text', fontsize=16)\n",
        "ax1.axis('off')\n",
        "\n",
        "ax2.imshow(wordcloud_negative, interpolation=\"bilinear\")\n",
        "ax2.set_title('Negative Reviews - Cleaned Text', fontsize=16)\n",
        "ax2.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nWdfMCeOamu_"
      },
      "outputs": [],
      "source": [
        "# Detailed analysis of the best model (SVM)\n",
        "print(\"=\" * 60)\n",
        "print(\"DETAILED ANALYSIS - LINEAR SVM (BEST PERFORMER)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(classification_report(lb_test_sentiments, svm_predict_fixed, target_names=lb.classes_))\n",
        "\n",
        "# Confusion matrix for SVM\n",
        "cm_svm = confusion_matrix(lb_test_sentiments, svm_predict_fixed)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=lb.classes_, yticklabels=lb.classes_)\n",
        "plt.title('Confusion Matrix - Linear SVM (Best Model)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Calculate key metrics\n",
        "tn, fp, fn, tp = cm_svm.ravel()\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "print(f\"\\nKey Metrics for SVM:\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-yTKusaZPd_"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9U0z5yWzZPd_"
      },
      "source": [
        "**Step 3:** (Extra Points: Optional!!)\n",
        "If you are able to complete this step and the remaining ones correctly, we will endorse your data science skills on LinkedIn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R6pjrddZPeA"
      },
      "source": [
        "Make a visualization of your choice! This is your chance to show your creativity and read about different visualization techniques. You can use the `matplotlib` package to make a visualization of your choice. You can also use the `seaborn` package to make a visualization of your choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn1r4y73ZPeA"
      },
      "source": [
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT8Rpdt4ZPeA"
      },
      "source": [
        "<h3 align = 'center' >\n",
        "Thank you for completing the project!\n",
        "</h3>\n",
        "\n",
        "Please do reach out to us if you have any questions or concerns. We are here to help you learn and grow.\n",
        "\n",
        "If you have any queries, please contact the NSDC HQ Team at nsdc@nebigdatahub.org.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "58fcf84e0ebbaad2b6cc744ed9d48691de4e147b5be28c3707516d96647d7374"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}